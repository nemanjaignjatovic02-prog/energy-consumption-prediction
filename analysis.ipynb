{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26facfe4",
   "metadata": {},
   "source": [
    "Finding Min and Max electricity consumption values, to later de-scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd245753",
   "metadata": {},
   "source": [
    "Initializing data and reading from dataset .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d4ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/15 11:55:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, hour, dayofweek\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EnergyConsumptionPrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_path = \"data/smart_meter_data.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339713a",
   "metadata": {},
   "source": [
    "Splitting the data, training and evaluating linear regression and random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d378a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 11:55:51 WARN Instrumentation: [05069226] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/10/15 11:55:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/10/15 11:55:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/10/15 11:55:51 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/10/15 11:55:54 WARN DAGScheduler: Broadcasting large task binary with size 1560.3 KiB\n",
      "25/10/15 11:55:54 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/10/15 11:55:55 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/10/15 11:55:55 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "25/10/15 11:55:56 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "25/10/15 11:55:56 WARN DAGScheduler: Broadcasting large task binary with size 1024.5 KiB\n",
      "25/10/15 11:55:57 WARN DAGScheduler: Broadcasting large task binary with size 10.5 MiB\n",
      "25/10/15 11:55:57 WARN DAGScheduler: Broadcasting large task binary with size 1099.2 KiB\n",
      "25/10/15 11:55:58 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "25/10/15 11:55:58 WARN DAGScheduler: Broadcasting large task binary with size 1101.5 KiB\n",
      "25/10/15 11:55:58 WARN DAGScheduler: Broadcasting large task binary with size 15.9 MiB\n",
      "25/10/15 11:55:59 WARN DAGScheduler: Broadcasting large task binary with size 1038.0 KiB\n",
      "25/10/15 11:55:59 WARN DAGScheduler: Broadcasting large task binary with size 18.3 MiB\n",
      "25/10/15 11:56:00 WARN DAGScheduler: Broadcasting large task binary with size 20.6 MiB\n",
      "25/10/15 11:56:02 WARN DAGScheduler: Broadcasting large task binary with size 22.4 MiB\n",
      "25/10/15 11:56:03 WARN DAGScheduler: Broadcasting large task binary with size 23.9 MiB\n",
      "25/10/15 11:56:04 WARN DAGScheduler: Broadcasting large task binary with size 25.0 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATION (test set) ===\n",
      "Linear Regression -> RMSE: 0.156323, MAE: 0.125980, R2: 0.095517\n",
      "Random Forest     -> RMSE: 0.160367, MAE: 0.129234, R2: 0.048116\n",
      "\n",
      "Linear Regression intercept: 0.23270551826669847\n",
      "Linear Regression coefficients:\n",
      "  Temperature: -0.0081487894783487\n",
      "  Humidity: -0.027358389912020133\n",
      "  Wind_Speed: -0.005548839197460918\n",
      "  Avg_Past_Consumption: 0.35379170057845083\n",
      "  hour: -0.00021103810694142737\n",
      "  dayofweek: 0.0011126025948691954\n",
      "\n",
      "Random Forest feature importances:\n",
      "  Temperature: 0.181098\n",
      "  Humidity: 0.156231\n",
      "  Wind_Speed: 0.167586\n",
      "  Avg_Past_Consumption: 0.233029\n",
      "  hour: 0.158428\n",
      "  dayofweek: 0.103629\n",
      "\n",
      "Sample predictions (Random Forest):\n",
      "+-------------------+-------------------+-------------------+\n",
      "|Timestamp          |label              |prediction         |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2024-03-13 22:00:00|0.5717734854086908 |0.46547477425019823|\n",
      "|2024-03-13 22:30:00|0.27044775900811624|0.3939755378933961 |\n",
      "|2024-03-13 23:00:00|0.3825371181666638 |0.43043411165301854|\n",
      "|2024-03-13 23:30:00|0.3835329836468476 |0.41875253808393353|\n",
      "|2024-03-14 00:00:00|0.291369301198078  |0.43274726494878657|\n",
      "|2024-03-14 00:30:00|0.14625574201931657|0.3469716289133567 |\n",
      "|2024-03-14 01:00:00|0.42882908393627805|0.35798783201372364|\n",
      "|2024-03-14 01:30:00|0.36483267135743613|0.3488761530315496 |\n",
      "|2024-03-14 02:00:00|0.4583734115727617 |0.40629717828576856|\n",
      "|2024-03-14 02:30:00|0.28606661559347224|0.35288447170714393|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Sample predictions (Linear Regression):\n",
      "+-------------------+-------------------+-------------------+\n",
      "|Timestamp          |label              |prediction         |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2024-03-13 22:00:00|0.5717734854086908 |0.4293471256222113 |\n",
      "|2024-03-13 22:30:00|0.27044775900811624|0.42691808467761394|\n",
      "|2024-03-13 23:00:00|0.3825371181666638 |0.41478792471564224|\n",
      "|2024-03-13 23:30:00|0.3835329836468476 |0.42574236890001027|\n",
      "|2024-03-14 00:00:00|0.291369301198078  |0.41549655617079506|\n",
      "|2024-03-14 00:30:00|0.14625574201931657|0.357372266761562  |\n",
      "|2024-03-14 01:00:00|0.42882908393627805|0.3481766667713254 |\n",
      "|2024-03-14 01:30:00|0.36483267135743613|0.3543888292979956 |\n",
      "|2024-03-14 02:00:00|0.4583734115727617 |0.3694922629407058 |\n",
      "|2024-03-14 02:30:00|0.28606661559347224|0.3540557410431014 |\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 11:56:08 WARN TaskSetManager: Stage 99 contains a task of very large size (1624 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1) Add hour and dayofweek features\n",
    "df = df.withColumn(\"hour\", hour(col(\"Timestamp\"))).withColumn(\"dayofweek\", dayofweek(col(\"Timestamp\")))\n",
    "\n",
    "# 2) Cast numerical columns to DoubleType\n",
    "feature_cols = [\"Temperature\", \"Humidity\", \"Wind_Speed\", \"Avg_Past_Consumption\", \"hour\", \"dayofweek\"]\n",
    "for c in feature_cols + [\"Electricity_Consumed\"]:\n",
    "    df = df.withColumn(c, col(c).cast(DoubleType()))\n",
    "\n",
    "# Drop rows with nulls\n",
    "df = df.dropna(subset=feature_cols + [\"Electricity_Consumed\", \"Timestamp\"])\n",
    "\n",
    "# 3) Build feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(df).select(\"features\", col(\"Electricity_Consumed\").alias(\"label\"), \"Timestamp\")\n",
    "\n",
    "# 4) Train-test split (70-30)\n",
    "data_ordered = data.orderBy(\"Timestamp\")\n",
    "count = data_ordered.count()\n",
    "split_idx = int(count * 0.7)\n",
    "train = data_ordered.limit(split_idx)\n",
    "test = data_ordered.subtract(train)\n",
    "\n",
    "# 5) Train Linear Regression \n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", regParam=0.0)\n",
    "lr_model = lr.fit(train)\n",
    "lr_pred = lr_model.transform(test)\n",
    "\n",
    "# 6) Train Random Forest\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=50,\n",
    "    maxDepth=20,\n",
    "    minInstancesPerNode=1,\n",
    "    seed=42\n",
    ")\n",
    "rf_model = rf.fit(train)\n",
    "rf_pred = rf_model.transform(test)\n",
    "\n",
    "# 7) Evaluation\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae  = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Linear Regression metrics\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_pred)\n",
    "lr_mae  = evaluator_mae.evaluate(lr_pred)\n",
    "lr_r2   = evaluator_r2.evaluate(lr_pred)\n",
    "\n",
    "# Random Forest metrics\n",
    "rf_rmse = evaluator_rmse.evaluate(rf_pred)\n",
    "rf_mae  = evaluator_mae.evaluate(rf_pred)\n",
    "rf_r2   = evaluator_r2.evaluate(rf_pred)\n",
    "\n",
    "print(\"=== EVALUATION (test set) ===\")\n",
    "print(f\"Linear Regression -> RMSE: {lr_rmse:.6f}, MAE: {lr_mae:.6f}, R2: {lr_r2:.6f}\")\n",
    "print(f\"Random Forest     -> RMSE: {rf_rmse:.6f}, MAE: {rf_mae:.6f}, R2: {rf_r2:.6f}\")\n",
    "\n",
    "\n",
    "# 8) Coefficients / Feature Importances\n",
    "print(\"\\nLinear Regression intercept:\", lr_model.intercept)\n",
    "lr_coeffs = lr_model.coefficients.toArray() if hasattr(lr_model, \"coefficients\") else lr_model.coefficients\n",
    "print(\"Linear Regression coefficients:\")\n",
    "for f, coef in zip(feature_cols, lr_coeffs):\n",
    "    print(f\"  {f}: {coef}\")\n",
    "\n",
    "print(\"\\nRandom Forest feature importances:\")\n",
    "rf_importances = rf_model.featureImportances\n",
    "for f, imp in zip(feature_cols, rf_importances):\n",
    "    print(f\"  {f}: {imp:.6f}\")\n",
    "\n",
    "# 9) Sample predictions\n",
    "print(\"\\nSample predictions (Random Forest):\")\n",
    "rf_pred.select(\"Timestamp\", \"label\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "print(\"\\nSample predictions (Linear Regression):\")\n",
    "lr_pred.select(\"Timestamp\", \"label\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# 10) Save models\n",
    "lr_model.write().overwrite().save(\"models/lr_model\")\n",
    "rf_model.write().overwrite().save(\"models/rf_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4affa60b",
   "metadata": {},
   "source": [
    "Saving results to be shown on the front-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42e41fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results successfully saved at fastapi_app/spark_results.json\n"
     ]
    }
   ],
   "source": [
    "# Creating a results dictionary\n",
    "results = {\n",
    "    \"LinearRegression\": {\n",
    "        \"RMSE\": lr_rmse,\n",
    "        \"MAE\": lr_mae,\n",
    "        \"R2\": lr_r2,\n",
    "        \"Coefficients\": {f: float(c) for f, c in zip(feature_cols, lr_coeffs)},\n",
    "        \"Intercept\": float(lr_model.intercept)\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"RMSE\": rf_rmse,\n",
    "        \"MAE\": rf_mae,\n",
    "        \"R2\": rf_r2,\n",
    "        \"FeatureImportances\": {f: float(imp) for f, imp in zip(feature_cols, rf_importances)}\n",
    "    }\n",
    "}\n",
    "\n",
    "# JSON path\n",
    "output_path = os.path.join(\"fastapi_app\", \"spark_results.json\")\n",
    "\n",
    "# Writing data into JSON file\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Results successfully saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb628d3",
   "metadata": {},
   "source": [
    "Preparing model data to be shown on the front-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24245930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from pandas import to_datetime\n",
    "\n",
    "# Converting spark dataframes to pandas for plotting\n",
    "lr_pred_pd = lr_pred.withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"string\")).toPandas()\n",
    "rf_pred_pd = rf_pred.withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"string\")).toPandas()\n",
    "\n",
    "# Converting Timestamp column back to datetime\n",
    "lr_pred_pd[\"Timestamp\"] = to_datetime(lr_pred_pd[\"Timestamp\"])\n",
    "rf_pred_pd[\"Timestamp\"] = to_datetime(rf_pred_pd[\"Timestamp\"])\n",
    "\n",
    "# Linear Regression prediction vs actual\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(lr_pred_pd[\"Timestamp\"], lr_pred_pd[\"label\"], label=\"Actual Consumption\", color=\"blue\")\n",
    "plt.plot(lr_pred_pd[\"Timestamp\"], lr_pred_pd[\"prediction\"], label=\"LR Prediction\", color=\"red\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Scaled Consumption (`%` of max = 500 kWh)\")\n",
    "plt.title(\"Linear Regression: Prediction vs Actual Consumption\")\n",
    "plt.legend()\n",
    "\n",
    "# format X-axis: max 10 ticks for readability\n",
    "plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator(maxticks=10))\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fastapi_app/images/lr_prediction.png\")\n",
    "plt.close()\n",
    "\n",
    "# Random Forest prediction vs actual\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(rf_pred_pd[\"Timestamp\"], rf_pred_pd[\"label\"], label=\"Actual Consumption\", color=\"blue\")\n",
    "plt.plot(rf_pred_pd[\"Timestamp\"], rf_pred_pd[\"prediction\"], label=\"RF Prediction\", color=\"green\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Scaled Consumption (`%` of max = 500 kWh)\")\n",
    "plt.title(\"Random Forest: Prediction vs Actual Consumption\")\n",
    "plt.legend()\n",
    "\n",
    "# Again, formatting X-axis: max 10 ticks for readability\n",
    "plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator(maxticks=10))\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fastapi_app/images/rf_prediction.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
